{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e68ef8-2d9d-4629-889b-d8be44f989e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d58952-7416-44cc-82d1-c34e8db9d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-79588ac3-7f54-3e95-8937-52f6c5ecb2ea)\n",
      "GPU 1: NVIDIA A100-SXM4-40GB (UUID: GPU-52fe6980-8833-8c84-2c21-28b6c5f24e62)\n",
      "GPU 2: NVIDIA A100-SXM4-40GB (UUID: GPU-61a613b9-ac5b-6169-0007-ebd14aa23d2b)\n",
      "GPU 3: NVIDIA A100-SXM4-40GB (UUID: GPU-4391586d-049e-34b3-411b-fe2134cc7e5c)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L # list GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7065dc3-2e64-48c9-8967-7dbc5d17438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPU Setup\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # pick a number < 4 on ML4HEP; < 3 on Voltan \n",
    "#physical_devices = tf.config.list_physical_devices('GPU') \n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f415bd-0bff-4b7a-9b97-6d38aa444a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5460e2-4d5d-422f-a96f-2bdd99576ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvolutionLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        # node_repesentations shape is [num_nodes, representation_dim]\n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(\n",
    "            node_indices, neighbour_messages, node_repesentations\n",
    "        )\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4e379f-016a-4569-99cf-deffa00a595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNetwork(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        #graph_info,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphNetwork, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        #node_features, edges, edge_weights = graph_info\n",
    "        #self.node_features = node_features\n",
    "        #self.edges = edges\n",
    "        #self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        #if self.edge_weights is None:\n",
    "        #    self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        #self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvolutionLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvolutionLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        #self.compute_logits = layers.Dense(units=len(self.node_features[0]), name=\"logits\")\n",
    "        self.compute_logits = layers.Dense(units=feature_dim, name = \"logits\")\n",
    "\n",
    "    def call(self, graph_info):\n",
    "        node_features = graph_info[0]\n",
    "        edges = graph_info[1]\n",
    "        edge_weights = graph_info[2]\n",
    "        edge_weights = edge_weights / tf.math.reduce_sum(edge_weights)\n",
    "\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, edges, edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, edges, edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        #node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        #return self.compute_logits(node_embeddings)\n",
    "        return self.compute_logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77ed703-3b72-4e7e-bbf9-f44d9004b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a fully connected graph with a specified number of nodes and features sampled from a random uniform distribution\n",
    "\n",
    "Node features output with shape [number of nodes, node feature dimension]\n",
    "Edges outputs with shape [2, number of edge connections]\n",
    "Edge weights output with shape [number of edge connections]\n",
    "\"\"\"\n",
    "def create_graph(node_count, feature_dim):\n",
    "    adjacency_matrix = [[], []]\n",
    "    for i in range(node_count):\n",
    "        for j in range(node_count):\n",
    "            if i != j:\n",
    "                adjacency_matrix[0].append(i)\n",
    "                adjacency_matrix[1].append(j)\n",
    "\n",
    "    edges = tf.constant(adjacency_matrix)\n",
    "    edge_weights = tf.ones(shape=edges.shape[1])\n",
    "    node_features = tf.constant(np.random.exponential(1.0,\n",
    "                    size = (node_count, feature_dim)) * np.random.choice([-1, 1], size = (node_count, feature_dim)) + 2.)\n",
    "    #node_features = tf.random.uniform(shape=[node_count, feature_dim])\n",
    "\n",
    "    return [node_features, edges, edge_weights]\n",
    "\n",
    "\n",
    "def diffuse_graph(node_features, beta, iterations = 1):\n",
    "    if type(node_features) is list:\n",
    "        node_features = node_features[0]\n",
    "\n",
    "    if len(node_features) != 0:\n",
    "        feature_dim = len(node_features[0])\n",
    "    else:\n",
    "        return node_features\n",
    "\n",
    "    updated_node_features = []\n",
    "    for i in range(len(node_features)):\n",
    "        noise = tf.random.normal([feature_dim])\n",
    "        updated_node_features.append(np.sqrt(1. - beta) * tf.cast(node_features[i], dtype = tf.float32) + np.sqrt(beta) * noise)\n",
    "\n",
    "    if iterations > 0:\n",
    "        return diffuse_graph(tf.stack(updated_node_features), beta, iterations - 1)\n",
    "    else:\n",
    "        return tf.stack(updated_node_features)\n",
    "\n",
    "\n",
    "def permute_graph(graph_info, permutation):\n",
    "    graph_info[0] = tf.gather(graph_info[0], permutation)\n",
    "    edges = graph_info[1]\n",
    "    edge_weights = graph_info[2]\n",
    "    permuted_edges = [[], []]\n",
    "    permuted_edge_weights = []\n",
    "    for i in range(len(edges[1])):\n",
    "        permuted_edges[0].append(permutation[edges[0][i]])\n",
    "        permuted_edges[1].append(permutation[edges[1][i]])\n",
    "        #permuted_edge_weights.append(permutation[edge_weights[i]])\n",
    "\n",
    "    graph_info[1] = tf.stack(permuted_edges)\n",
    "    #graph_info[2] = tf.stack(permuted_edge_weights)\n",
    "\n",
    "\n",
    "def generate_graph_ensemble(num_graphs, node_count, feature_dim):\n",
    "    graphs = []\n",
    "    for i in range(num_graphs):\n",
    "        graphs.append(create_graph(node_count, feature_dim))\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def ensemble_statistics(graphs, node_index):\n",
    "    feature_averages = np.zeros([len(graphs[0][0][0])])\n",
    "    for graph in graphs:\n",
    "        node_feature = graph[0][node_index]\n",
    "        for i in range(len(node_feature)):\n",
    "            feature_averages[i] += node_feature[i]\n",
    "\n",
    "    feature_averages /= len(graphs)\n",
    "    return feature_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d22700c-18f1-43dd-8ece-fec393d975a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 17:17:14.743463: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 17:17:16.021777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38270 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2022-11-06 17:17:16.023138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38270 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2022-11-06 17:17:16.024751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38270 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "graph_info = create_graph(4, 3)\n",
    "\n",
    "hidden_units = [8]\n",
    "dropout_rate = 0.5\n",
    "\n",
    "gnn_model = GraphNetwork(\n",
    "    #graph_info=graph_info,\n",
    "    feature_dim=3,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27233c51-ff1f-4bcb-8eda-56574ecc6b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphs = generate_graph_ensemble(2000, 4, 3)\n",
    "#print(ensemble_statistics(graphs, 0))\n",
    "\n",
    "feature_values = list(map(lambda graph: float(graph[0][0][0]), graphs))\n",
    "#print(feature_values)\n",
    "\n",
    "for i in range(len(graphs)):\n",
    "    graphs[i][0] = diffuse_graph(graphs[i][0], 0.1, 50)\n",
    "\n",
    "diffused_feature_values = list(map(lambda graph: float(graph[0][0][0]), graphs))\n",
    "\n",
    "plt.hist(feature_values, 100, range = (-5, 5), alpha = 0.5)\n",
    "plt.hist(diffused_feature_values, 100, range = (-5, 5), alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14e2ad-e7b4-436c-8f3f-777a187883d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
